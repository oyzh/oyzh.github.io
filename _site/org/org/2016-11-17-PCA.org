#+BEGIN_HTML
---
layout: post
title: 主成分分析(PCA)
excerpt: 介绍PCA
---
#+END_HTML
#+OPTIONS: toc:nil
#+OPTIONS: ^:{} 
#+OPTIONS: num:nil
主成份分析(PCA)是一种数据降维的方法,比如得到一批输入数据 $X_{N \times M}$ ( $X$ 是 $N \times M$ 的矩阵),
其中 $M$ 表示样本个数, $N$ 表示单个样本的特征个数.我们想要的结果是
$X_{N' \times M}^{'}$ ,其中 $N'$ 比 $N'$ 要小,同时不会丢失太多信息.

* 基础知识
** 线性算子
   矩阵乘表示的是一种线性的空间变换,线性算子是输入和输出在同一个空间的变换,
   在我们想用PCA时,我们的目的是要找一个线性算子 $P_{N \times N}$ ,使得变换后
   的feature有比原来更好的性质.
** SVD
   在PCA中需要求协方差矩阵然后求协方差矩阵的特征值和特征向量，而利用SVD可以通过分解 $X^T$ 来计算，不用直接求协方差矩阵。

   SVD是对于任意的矩阵A(m×n)有：
   $$A=USV^T$$
   其中有 $U^T \times U=E$ 和 $V^T \times V=E$ 。具体计算可以参考书本。
** 统计
   假设样本的采集随机的,那么特征向量每一维是一个随机变量,一共有 $N$ 个随机变量,
   这些特征之间可能会有相关性(比如有两个特征 $x,y$ ,每个样本 $y$ 的取值都是 $x$
   取值的两倍,这两个变量之间就有相关性,去掉其中一个完全不影响我们得到的信息),
   协方差矩阵刚好可以描述任意两个变量之间的方差.我们的目的就是变换后的数据两个
   随机变量之间的方差变为0.协方差的计算是:
   $$\Sigma_{i=1}^M(x_i-E(X))(y_i-E(Y))$$
   如果我们预处理让每一个特征减去均值,则期望变为0,则公式直接变成内积.下文都按照已经预处理.
* 算法
  我们要找一个矩阵 $P_{N \times N}$ 来变换每个样本的特征,可以知道,变换后的所有的样本和特征是:
  $$ X' = PX $$
  我们想要X'的协方差只有对角线有值,及变量之间没有相关性.
  $$ X' \cdot X^{'T} = PX(X^TP^T)$$
  $$ = P(XX^T)P^T$$
  用SVD分解中间部分:
  $$ = P(USVV^TS^TU^T)P^T$$
  其中 $SS^T$是特征值平方的只有斜线不为0的矩阵,记为 $C$,
  $$ = PUCU^TP^T$$
  其中 $U^T = U^{-1}$,所以有
  $$ P = U^{T}$$
  这样直接对 $X$ 进行SVD分解就可以求得 $P$,假设 $C$是按照方差依次减小排列的,
  方差越小表示数据的区分度不大,因此这一维度的特征对整体区分数据也就不大,因此我们
  可以取变换后的 $X'$ 的前面部分特征来实现降低维度.

  Matlab的测试代码:
#+BEGIN_SRC
a = [1:10];
b = 2*a;
% 10个样本,每个样本俩个特征
X = [a;b];

% 减去均值
X = X - repmat(mean(X,1),[2,1]);

% svd分解
[U,S,V] = svd(X);

P = U';
% 变换后的数据
X2 = P*X;
% X的值
% -0.5000   -1.0000   -1.5000   -2.0000   -2.5000   -3.0000   -3.5000   -4.0000   -4.5000   -5.0000
%  0.5000    1.0000    1.5000    2.0000    2.5000    3.0000    3.5000    4.0000    4.5000    5.0000
% 求得X2的值,第二部分已经都变为0了
%  0.7071    1.4142    2.1213    2.8284    3.5355    4.2426    4.9497    5.6569    6.3640    7.0711
%  0.0000    0.0000    0.0000    0.0000    0.0000    0.0000    0.0000    0.0000    0.0000    0.0000

plot(X(1,:),X(2,:));
plot(X2(1,:), X2(2,:));
#+END_SRC
