#+BEGIN_HTML
---
layout: post
title: CART分类回归树
excerpt: classification and regression tree,分类回归树的理论
---
#+END_HTML
#+OPTIONS: toc:nil
#+OPTIONS: ^:{}


* 决策树
  决策树的本质是找出适合的规则来进行分类和回归,
将输入空间进行划分并对每一个子空间赋值.这样的规则是简单的,
每次只测试一个属性,对于 $n$ 个属性的输入:
$X=(x_1,x_2,\dots,x_n)$ 树的每个节点测试一个属性,
比如 $x_1=10$ ,这样的处理可以想象输入空间是被很多垂直的面划分的,
我认为这是决策树的一个缺点,
它每次不能联系多个属性进行决策,
比如 $x_1 + x_2 = 10$ 可能是更好的划分,它在空间上这是一条"斜线",
这在决策树中是不可能出现的.

如果我们要构建决策树,作为一个递归的过程,
每次的任务变成了如何选取一个属性(和这个属性的一个值,CART要用)
来将数据进行"好"的分割,所以量化一个分割的好坏是决策树的核心.

* 属性选择依据
  上面说到每次要选择一个属性 $x$ (和分割 $s$ )来划分数据,直观上如果选择了这样的
$x$ 将数据分成了几个子区域,每个子区域的label如果越"混乱",则说明这个划分不好,
如果每个子划分里的label越"统一",则说明这个划分越好.因此我们的任务变成了量化
划分后的数据的"混乱程度".

注意这用到的其实是贪心算法,每次找最好的划分,但是结果可能不是全局最好的划分.
** 信息增益
   用信息增益可以量化一个划分的好坏.首先要用熵(entropy)的概念,简单来说熵量化了
一个概率分布的不确定性.我们可以计算每一个划分后的区域的熵,相加的和表示了用这个
$x$ 和 $s$ 划分后的所有子区域总的混乱程度之和,测试所有可能的 $x$ 和它所有可能的分割值$s$,
找出划分后混乱最小的属性作为当前的分割值.

** ID3和C4.5算法
   这两个算法用上面的找属性的方法来构造决策树,区别只在于用来量化分割好坏的指标.
ID3用的信息增益,C4.5用的是信息增益比,之间区别比较小

* CART算法
  CART算法可以用来做分类和回归,CART简单在于它是二叉树,它的每个节点测试一个属性条件,
左右子树对应测试对和测试错的分支.归根结底我们还是用贪心的方式每次找出一个最好的划分.
不同的判断方法对应不同算法

** 最小二乘回归树
   如果输入训练数据D,递归划分使得我们每次只用考虑如何选取一个属性$j$和切分点$s$来使划分点
最好.给定一个区域的数据,将这个区域的取值定为区域内的label的平均值时可以使得二次距离平方和最小
(这里假设了我们用最小二乘来判断好坏,直接可得解析解,其他的距离函数可能会让我们赋值时有不同的选择!!)
比如对于输入$$D={(X_1,Y_1),(X_2,Y_2),\dots,(X_n,Y_n)}$$
假设某种划分将D分成了M个单元$R_1,R_2,\dots,R_M$,我们可以给第m个区域赋值:
$$\hat{c}_m=average(Y_i|X_i \in R_m)$$
这时的
$$\Sigma_{X_i\in R_m}(Y_i-c)^2$$取得最小值.

下一步要遍历所有的属性和可能的切割点,具体就是优化下式:
$$min_{j,s}[\Sigma_{x_i\in{R_1(j,s)}}(y_i-c_1)^2+\Sigma_{x_i\in{R_2(j,s)}}(y_i-c_2)^2]$$
其中 $c_1$ 和 $c_2$ 是上面所说的平均值.

递归的进行构造得到整个决策树.

** 分类树
   这里用基尼(Gini)指数来选择最优特征点.
