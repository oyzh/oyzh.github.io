#+BEGIN_HTML
---
layout: post
title: 时间复杂度计算
excerpt: 递归函数的计算,用于程序的时间复杂度分析(算法导论第一部分)
---
#+END_HTML
#+OPTIONS: toc:nil
#+OPTIONS: ^:{}

* 时间复杂度
  假设输入问题的规模为 $n$ ,定义时间复杂为 $T(n)$ ,时间复杂度是关于输入规模的函数, 非递归定义的函数比较好估计,递归定义的函数需要技巧来估计. 常见的比如快排,归并排序等的时间函数都是递归定义的.
比如归并排序,当 $n=1$ 时, $T(n)=c$ ,当 $n>1$ 时, $T(n) = T(\lceil n/2 \rceil) + T(\lfloor n/2 \rfloor) + cn$ .

我们研究的是算法的 *渐近* 效率,即不用求的精确的 $T(n)$ ,只需得到它的一个增长量级.运行时间是 $n$ 的函数,我们可以定义
函数集合来使我们的时间函数属于某个函数集合.某个函数集合反映了一类时间复杂度,比如 $\Theta(n)$ 表示时间复杂度是 $n$ 的
函数集合.具体定义如下.

* 定义
** $\Theta$ 记号
   对于一个给定的函数 $g(n)$ ,用 $\Theta(g(n))$ 来表示以下的函数集合:

$\Theta(g(n))$ = {$f(n)$:存在正的常数$c_1$, $c_2$ 和 $n_0$ ,使得对于所有$n\ge n_0$,有 $0\le c_1g(n) \le f(n) \le c_2g(n)$ }

即存在正的常数 $c_1$ 和 $c_2$ ,使得对于足够大的 $n$ , 函数 $f(n)$ 能 *夹入* $c_1g(n)$ 与 $c_2g(n)$ 之间,则 $f(n)$ 属于集合 $\Theta(g(n))$ .

这里的 $g(n)$ 是 $f(n)$ 的一个 *渐近紧确界* .
** $O$ 和 $o$ 记号
$O(g(n))$ 表示的是渐近上界,它可能是紧确的,也可能不是,而 $o(g(n))$ 一定不是上紧确的.

$\O(g(n))$ = {$f(n)$:存在正的常数 $c$ 和 $n_0$ ,使得对于所有 $n \ge n_0$ ,有 $0\le f(n) \le cg(n)$ }

$\o(g(n))$ = {$f(n)$:存在正的常数 $c$ 和 $n_0$ ,使得对于所有 $n \ge n_0$ ,有 $0\le f(n) < cg(n)$ }

** $\Omega$ 和 $\omega$ 记号
$\Omega(g(n))$ 表示下界,可能是紧确的,也可能不是,而 $\omega$ 一定不是紧确的.

$\Omega (g(n))$ = {$f(n)$:存在正的常数 $c$ 和 $n_0$ ,使得对于所有 $n \ge n_0$ ,有 $0 \le g(n) \le f(n)$ }

$\omega (g(n))$ = {$f(n)$:存在正的常数 $c$ 和 $n_0$ ,使得对于所有 $n \ge n_0$ ,有 $0 \le g(n) < f(n)$ }

* 计算
对于给定的递归的函数,我们可以计算它的时间复杂度的界.通常有3中方法(第四章):
1. 猜测一个解,并用代入法求解
2. 用递归树方法求解
3. 用主方法求解

方法1需要猜测一个解,然后代入递归式,假设小于n都成立,证明等于n时也成立,通常需要猜测一个好的解,而方法2可以用来猜测这个
解.用递归树有时可以得到精确的时间函数,这是我们可以判断得到的是一个紧的界.但有时由于树是非完全树等原因,通常我们会放宽条件,
得到的可能是一个不紧的界,这是就需要用代入法来验证.方法3可以解决一类递归式,但是还有很大一部分不能用这个公式.

* 证明中常用公式和实例
** 常用公式
*** 取整公式
有些递归式带有取整符号,在用代入法求解时通常需要"凑",不如证明 $O(n^2)$ 时可以转化成证明 $O(n^2-1)$ ,这两个是一样的.
下面几个公式比较常用:
$$ \lceil \frac{n}{2} \rceil + 1 \ge \frac{n+1}{2}$$
$$ \lfloor \frac{n}{2} \rfloor + 1 \ge \frac{n+1}{2}$$
$$ \lceil \frac{n}{2} \rceil - 1 \le \frac{n-1}{2}$$
$$ \lfloor \frac{n}{2} \rfloor - 1 \le \frac{n-1}{2}$$

*** 调和级数
调和级数也比较容易出现:
$$\Sigma_{i=1}^{n} \frac{1}{i} = \ln{n} + O(1)$$

*** 阶乘的界
阶乘 $n!$ 可以给出一个弱上界 $n! \le n^n$ ,斯特林(Stirling)公式给出了一个更紧确的上下界:
斯特林(Stirling)近似公式:
$$n! = \sqrt{2 \pi n}(\frac{n}{e})^n(1+\Theta(\frac{1}{n}))$$
可以借助斯特林公式证明:
$$\lg(n!)=\Theta(n\lg n)$$

*** 求根深度
在做递归树展开时,通常需要计算树的深度,一般的比较容易,而想 $T(\sqrt{n})$ 的深度可以由以下估计,首先假设最后结果是2,证明如下:
$$ \sqrt[2^k]{n} = 2 $$
$$ n = 2^{2^k}$$
$$ k = \lg{\lg n} $$
求得深度k

*** fibbonaci数
Fibbonaci有通项公式:
$$F_i = \frac{1}{\sqrt{5}}((\frac{1+\sqrt{5}}{2})^i - (\frac{1-\sqrt{5}}{2})^i)$$
由于后面那一项的绝对值小于1,所以更简单的公式:
$$F_i = round(\frac{1}{\sqrt{5}}(\frac{1+\sqrt{5}}{2})^i)$$
$round$ 表示舍入到最近的整数,可以用加二分之一再向下取整实现.

*** 量级比较
一些比较常见的量级比较:
+ 对于 $a>1$ 的实常量 $a$ 和 $b$ ,有:
$$n^b = o(a^n)$$
+ 对于任意常量 $a>0$ :
$$\lg^b{n} = o(n^a)$$
+ 对于 $1 \le a<b$ ,有:
$$na^n = o(b^n)$$
** 实例
* 概率分析和随机算法
** 期望的线性性质
两个随机变量的和的期望与他猛的期望的之和相等:
$$ E[X+Y] = E[X] + E[Y] $$
其中, $E[X]$ 与 $E[Y]$ 需有定义.我梦成这个性质为期望的线性性质,
*并且即使 $X$ 与 $Y$ 不独立,该性质也成立* . 这一性质可以扩展到有限的以及绝对收敛的期望和上.
期望的线性性质是允许我们使用指标随机变量进行概率分析的关键性质.

理解:期望的线性性质非常重要,主要是对于不独立的随机变量也是成立的,这和直觉优点不一致.
有几点需要注意:
1. 变量是加的形式
2. 变量之间可以不独立
因为求得是期望,而不是某个事件的概率.所以可以这样使用.

** 随机算法
概率分析: 指的是输入数据服从一个概率分布,如果我们知道这个概率分布,我们可以计算一个算法对这个分布产生的数据
的平均时间复杂度.

随机算法: 让随机发生在算法上,给定一个输入,我们多次运行这个算法的平均时间复杂度.

例如,对于快排来说,它最差的情况是 $O(n^2)$ ,如果一个输入是0到n的均匀随机的排列,
则如果有大量这个分布产生的数据输入到快排中,快排的期望的效率是 $O(n\lg{n})$ ,但是如果一个分布更倾向于
最差情况,则多个这个分布产生的数据输入快排的期望的效率是 $O(n^2)$ ,这时算法的效率依赖与输入的分布.

而随机算法是对于一个输入,不论是最差还是最好,如果对同一个输入运行多次,则期望都是一样,比如快排是 $O(n\lg{n})$,
因为算法对输入进行排列打乱.

理解:
如果我们知道输入的概率分布,我们可以针对这个分布进行优化得到最好的结果.但是如果我们对于输入未知,则可以使用随机算法.
随机算法能保证如果输入的分布是最差的分布,算法效率也不会最差.

这里我想到几个问题:
1. 如果知道输入的分布,怎样针对性的设计算法.比如如果知道输入数据大致(量化)排好序,用随机算法肯定不是最好的
2. 如何估计分布,如果我们可以估计分布,就可以结合分布的估计和针对特定分布的算法
3. 机器学习中无监督方法可以估计概率分布,是否可以用到这里来

** 证明产生的是均匀随机排列
随机算法重要的环节是打乱输入,这是我们要证明输入是一个均匀随机排列.即输入一个数组,我们要构建一个方法
构造这个数组的一个随机排列,且这个数组 *所有可能的排列出现的概率相等(均匀随机排列)* .
