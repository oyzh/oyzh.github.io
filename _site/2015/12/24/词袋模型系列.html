<!DOCTYPE html>
<html>
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
	<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.7.0/styles/default.min.css">
	<link rel="stylesheet" type="text/css" href="/styles/post.css">
    <script src="/scripts/jquery-3.1.1.js" type="text/javascript"></script>
	<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

    <script type="text/javascript">

    </script>
	<title>词袋模型及其改进</title>
  </head>

  <body>
    <header>
      <nav>
		<div id="mytitle">
    	          <a href="/index.html"><img src="/images/racket.png" height="50" width="50" alt="" /></a>
                  <a style="font-family:Source Code Pro,Monaco,Courier,Consolas;" href="index.html">(set 'name "oyzh")</a>
		</div>
		<a href="https://github.com/oyzh/oyzh.github.io"><img id="forkme" src="/images/fork_me.png"" alt="Fork me on GitHub"></a>
      </nav>
     </header>
	<article>
	<h1 id="articlename">词袋模型及其改进</h1>
		<ol class="org-ol"><li><a id="sec-1" name="sec-1"></a>BoF（bag of feature）<br  /><div class="outline-text-2" id="text-1">
<p>
词袋模型是从文本识别中借鉴到视觉中的方法。这种方法对文本建模时不考虑整体的结构，只是简单的根据单词出现的规律来判断文件类型。比如一共有m篇文章，每一篇有一个类别，像是新闻、娱乐或体育等。统计出现在这些文章中的所有词，把这些词作为一个字典，这里假设一共出现过n个词。之后统计每一篇文章的词的个数，每一篇文章可以构成一个n维向量。这样，一共有m个n维向量，并且有m个对应每一个向量的label。在得到m个向量后就可以构造分类器，并使用这个分类器来对新的文章生成的向量进行预测。对于新的要求分类的文章，我们做同样的操作，统计它在字典中每个词的个数，所有也构成n维向量用来预测。
</p>

<p>
视觉中使用的词袋模型和这个非常相似。首先是如何构造词典。可以用特征描述子来作为单词，像使用SIFT特征，但是这样空间太大，需要采用聚类的方法的到k个聚类，聚类方法会返回每一个类的中心点。每一个聚类作为一个“单词”，因此我们一共的到k个单词构成的字典。对于每一张图片可以的到一个k维的向量，具体过程是：计算图片的所有特征描述子，每一个描述子归于与之最近的一个类，这样k维向量的每一分量值代表图片在这个类中有描述子的个数。这样我们就将每一张图片建模为一个k维的向量。可以用l张训练照片得到l个k维向量，并且每一张照片有一个label，因此有l个label。使用这些数据进行训练，得到一个分类器。测试数据也计算出这样的一个k维向量，然后使用得到的分类器进行分类。
</p>

<p>
在构建这个模型中使用的工具有：k-means聚类算法、SIFT（或其他的生成描述子的算法如SURF）、SVM（或其他分类方法如knn）。
</p>

<p>
对这种方法还有许多改进和提升的方法，比如采用金字塔等。
</p>

<p>
可以参考<a href="https://en.wikipedia.org/wiki/Bag-of-words_model_in_computer_vision">wiki</a> 和后面的的References。
</p>

<p>
在实现词袋模型时有几点需要注意：
</p>
<ol class="org-ol">
<li>像CIFAR-10这样的数据集，图片非常小，因此提取的特征非常少，结果也比较差。还有一点就是整个图片都是物体本身，所有可以采用将图片分割或多少步求一个特征，这样会得到很多的特征。可以使用vl_sift。
</li>
<li>在用kmeans时必须自己决定聚类的个数，这是一个需要调的参数。或者采用可以自己决定聚类个数的其他kmeans算法。
</li>
<li>分类器：knn效果不好，可能需要多分类的SVM。
</li>
</ol>

<p>
参考论文：Visual Caegorization with Bags of Keypoints. G.Csurka,C. R. Dance,L. Fan,J. Willamowski,C. Bray.
</p>
</div>
</li>

<li><a id="sec-2" name="sec-2"></a>SPM（spatial pyramid matching）<br  /><div class="outline-text-2" id="text-2">
<p>
SPM算法是建立在词袋模型上的。首先词袋模型由于是整个图片的直方图，因此它丢失了局部的信息，因此SPM是建立金子塔将局部信息也编码进去。这里的金子塔不是降采样的金字塔，而是将图片分割。比如最底层是将图片分割层16份，上一层是分成4份，最上层是整张图片。对每一个小块都求自己块内的直方图，并按一定比例的加权构成一个大的向量作为描述向量。然后就是分类，这里采用的是金字塔匹配的特殊的方法，它是计算一个非线性的核用于SVM分类。这样做的目的是适应金字塔的结构，能得到比较好的结果。
</p>

<p>
例如：一张640×480的图片，采用16×16的块求SIFT特征，并且移动的步数是8个像素，一张图片要生成 \(M=(640-8)/8+(480-8)/8)\) 个128维的特征向量。将训练图片得到的所有特征向量用k-means聚类得到dictionary，假设有N个word。描述向量是从金字塔底层到高层，每一层的每一块都生成一个N维的向量，比如一共3层的话，最底层有16块，一共生成16×N长的向量，最后组成 \((16+4+1)*N\) 长的一幅图的描述向量。这样每一幅图都生成这样一个描述向量。之后是计算交叉核用于分类，而测试的图片也要和所有的原图向量计算分类的核。最后使用得到的交叉核来用SVM进行分类。
</p>

<p>
参考论文：Beyond Bags of Features:Spatial Pyramid Matching for Recognizing Natural Scene Categories. Svetlana Lazebnik, Cordelia Schmid, Jean Ponce.
</p>
</div>
</li>

<li><a id="sec-3" name="sec-3"></a>ScSPM（Sparse Coding Spatial Pyramid Matching）<br  /><div class="outline-text-2" id="text-3">
<p>
ScSPM是对SPM的一种改进，SPM的计算量非常大，因为都要计算交叉核。ScSPM是采用另一种方法编码，可以使用线性核得到很好的结果。
</p>

<p>
ScSPM是采用稀疏的编码方式。这和SPM采用的硬编码不一样，它是一种“软”编码，就像它可以通过不同的权重投票一样。因此它也不是采用k-means算法，而是另一个最小化函数（见论文）。接下来是pooling的过程，即将k个N维的软编码向量融合成一个N维的向量。这里采用的是最大值pooling，即向量某一位的值是k个向量对应位的值中最大的。而SPM或词袋模型中是是采用的统计平均值。这样像SPM中一样金字塔的向量拉成一个大的向量就是描述向量，并且采用线性核（论文）进行分类，这样训练和分类的效率非常高。
</p>

<p>
下图是SPM与ScSPM的区别：
<img src="../images/ScSPM.jpg" alt="ScSPM.jpg" />
</p>

<p>
参考论文：Linear Spatial Pyramid Matching Using Sparse Coding for Image Classification. Jianchao Yang,Kai Yu,Yihong Gong,Thomas Huang.
这篇论文的几个公式非常重要，对整个过程讲的也很清楚。
</p>
</div>
</li>

<li><a id="sec-4" name="sec-4"></a>总结<br  /><div class="outline-text-2" id="text-4">
<p>
这里是词袋模型发展过程中比较重要的几个工作，从没有局部信息的BoF到采用金子塔并使用金字塔匹配核来实现的SPM，再到ScSPM使用稀疏编码和最大值pooling，并使用线性SVM，整个过程都是围绕直方图展开。只是具体的操作比如如何生成字典，如何匹配等不一样。需要理清整个发展的方向。
</p>
</div>
</li></ol>


<!-- LiveRe City install code -->
<div id="lv-container" data-id="city" data-uid="MTAyMC8yNjk2NC8zNTUx">
<script type="text/javascript">
   (function(d, s) {
       var j, e = d.getElementsByTagName(s)[0];

       if (typeof LivereTower === 'function') { return; }

       j = d.createElement(s);
       j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
       j.async = true;

       e.parentNode.insertBefore(j, e);
   })(document, 'script');
</script>
<noscript>Please activate JavaScript for write a comment in LiveRe</noscript>
</div>
<!-- completed City install code -->

	</article>


	<footer>
	Copyright &#x000A9;&nbsp;2016 &#x000B7 <a href="/index.html">oyzh</a>
	</footer>
  </body>
	<script>$("pre").wrapInner("<code></code>");</script>
	<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.7.0/highlight.min.js"></script>
	<script>
		hljs.initHighlightingOnLoad();
	</script>
</html>
