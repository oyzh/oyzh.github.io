#+HTML_MATHJAX: align:"center" mathml:t path:"http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" indent: 0em 
#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="/css/style.css">
#+BEGIN_HTML
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({ TeX: { equationNumbers: {autoNumber: "AMS"} } });
</script>
#+END_HTML
#+OPTIONS: author:nil
#+OPTIONS: creator:nil
#+OPTIONS: timestamp:nil
#+OPTIONS: num:nil
-----
#+TITLE:Deep learning 论文翻译
#+OPTIONS: toc:nil
* 监督学习
不论是不是有深度的，最一般的机器学习的形式都是监督学习。假设我们想要构建一个系统能根据内容对图片进行分类，比如房子、车子、人或宠物。我们首先搜集一个大的房子、车、人和宠物的数据集，每一张照片被标记了它的类别。在训练期间，机器被输入一张图片并参生一个向量形式的得分输出，每个分数代表一个种类。我们想要需要的种类要在所有种类中得分最高，但在训练前是不太可能。我们计算一个目标函数来测量输出得分与需要的模式的得分之间的错误（或距离）。然后机器可以修改它内部的可调节参数(adjustable parameters)来降低这个错误。这些可调节参数通常被称为权重(weights），它们是可以被看作旋钮(knob)的来定义机器输入-输出函数的实数。在一个典型的深度学习系统中，可能有上百万的这样的可调节权重和数百万的已标记的例子来训练机器。

为了更好的调节权重向量，学习算法计算一个梯度向量(gradient vector) for each weight, indicates by what amount the error would increase or decrease if the weight were increased by atiny amount. The weight vector is then adjusted in the opposite direction to the gradient vector.

使用所有训练样例平均过后的目标函数可以被看作是权重值的高维向量的一个陡峭的分割面。负梯度向量指出了在这个分割面中最快下降的方向，使它靠近最小值，输出值的错误平均小来最小。

实际操作中，从业人员使用一种被称为随机梯度下降(SGD)的步骤。这个步骤包括输入几个例子的输入向量，计算输出和错误，计算这些例子的平均梯度和相应的调整权重。这个过程在许多来自训练集的例子集上重复知道目标函数的平均值停止下降。被称为随机是因为每一个小的例子集给出一个对所有例子的平均梯度的噪音估计。这个简单的处理方式通常能令人吃惊快的找到一个好的权重集合。训练以后，系统的性能在另一个称为测试集的的集合上测试。这种方法能测试机器的“泛化”能力（generalization ability），即对于一个没有在训练集中的新的输入产生好的答案的能力。

许多目前机器学习的实际应用是在手工生成的特征上使用线性分类器。一个二分类的线性分类器计算一个特征向量的分量的加权和。如果加权和在一个阈值之上，那么一个输入就被划分到属于某一个特定的类。

从1960年代我们已经知道线性分类器仅仅能将输入空间分割成简单的区域，即被一个超平面分开的半空间(half-spaces)。但是像图片和语音识别问题需要输入-输出函数对不输入的不相关不敏感，比如对象的位置、方向或光照变化，或者声音的音高和声调的变化。同时要对特定的微小变化敏感（例如一只白色的狼和一个饲养的像狼一样的白色Samoyed犬）。在像素级别，处于不同环境的两只有不同姿势的Samoyeds图片会非常不同。然而同样背景同样姿势的一只Samoyed和狼的两张照片可能会非常像。一个在原生像素上操作的线性分类器，或其他任何“浅层”（shallow）分类器都不可能区分后一种情况的两幅图，仅仅是将前两张图划分为一个类。这就是为什么浅层分类器需要一个好的特征提取器（feature extractor）来解决可选择-不变性（selectivity-invariance）难题，这个特征产生一种对图像某一方面的表示，这种表示对图像区分很重要，但是对不想关方面变化不产生变化，比如动物姿势变化。为了使分类器更强大，可以使用一般非线性特征（generic non-liner features），比如核方法(kernel methods），但是一般化的特征比如采用高斯核升维的特征不能使学习者对离训练数据很远的数据泛化的很好。传统的选择是手工设计好的特征提取器，这需要强大的工程技术和领域经验。但是如果好的特征能够使用一个通用目地（general-purpose）学习步骤来自动的学习，那么手工设计的步骤就可以避免。这是深度学习的主要优势。

一个深度学习的体系结构是一个由简单组件构成的多层堆，它们中所有（或大多数）是为了用来学习，并且大多数是计算非线性的输入输出映射。这个堆中的每一个组件变化它的输入来增加整体的可选择性和表示的不变性。使用多个非线性层，也就是5到20的深度的系统能对输入实现极端复杂的函数，这些输入同时具有对微小的细节敏感——从白色狼中区分Samoyeds——和对大的部相关变化不敏感，例如背景，姿势，光照和周围物体。

* 反响传播(Backpropagation)训练多层结构
模式识别的早期，研究人员的目标是使用可训练的多层神经网络来代替手工生成的特征，但是尽管它很简单，知道1980年代中期结果也没有被很好理解。正像显示的一样，多层结构可以使用简单的随机梯度下降来训练。只要这些组件对于它们的输入和内部权重是相对平滑的函数，就可以使用反响传播操作来计算梯度。这个方法可以使用并工作的很好的观点是从1970-1980年间不同的群体独立完成的。
-----
#+BEGIN_HTML
<a href="http://oyzh.github.io">Back to Homepage</a>
<br>
<a href="http://github.com/oyzh">Github</a>
#+END_HTML
