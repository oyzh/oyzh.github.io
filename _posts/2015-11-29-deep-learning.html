<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<title>Deep Learning</title>
<meta  http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta  name="generator" content="Org-mode" />
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center; }
  .todo   { font-family: monospace; color: red; }
  .done   { color: green; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  pre.src-sh:before    { content: 'sh'; }
  pre.src-bash:before  { content: 'sh'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-R:before     { content: 'R'; }
  pre.src-perl:before  { content: 'Perl'; }
  pre.src-java:before  { content: 'Java'; }
  pre.src-sql:before   { content: 'SQL'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.right  { text-align: center;  }
  th.left   { text-align: center;   }
  th.center { text-align: center; }
  td.right  { text-align: right;  }
  td.left   { text-align: left;   }
  td.center { text-align: center; }
  dt { font-weight: bold; }
  .footpara:nth-child(2) { display: inline; }
  .footpara { display: block; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  /*]]>*/-->
</style>
<link rel="stylesheet" type="text/css" href="/css/style.css">
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2013 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type="text/javascript">
<!--/*--><![CDATA[/*><!--*/
    MathJax.Hub.Config({
        // Only one of the two following lines, depending on user settings
        // First allows browser-native MathML display, second forces HTML/CSS
            config: ["MMLorHTML.js"], jax: ["input/TeX"],
        //  jax: ["input/TeX", "output/HTML-CSS"],
        extensions: ["tex2jax.js","TeX/AMSmath.js","TeX/AMSsymbols.js",
                     "TeX/noUndefined.js"],
        tex2jax: {
            inlineMath: [ ["\\(","\\)"] ],
            displayMath: [ ['$$','$$'], ["\\[","\\]"], ["\\begin{displaymath}","\\end{displaymath}"] ],
            skipTags: ["script","noscript","style","textarea","pre","code"],
            ignoreClass: "tex2jax_ignore",
            processEscapes: false,
            processEnvironments: true,
            preview: "TeX"
        },
        showProcessingMessages: true,
        displayAlign: "center",
        displayIndent: "2em",

        "HTML-CSS": {
             scale: 100,
             availableFonts: ["STIX","TeX"],
             preferredFont: "TeX",
             webFont: "TeX",
             imageFont: "TeX",
             showMathMenu: true,
        },
        MMLorHTML: {
             prefer: {
                 MSIE:    "MML",
                 Firefox: "MML",
                 Opera:   "HTML",
                 other:   "HTML"
             }
        }
    });
/*]]>*///-->
</script>
</head>
<body>
<div id="content">
<h1 class="title">Deep Learning</h1>
<hr  />
<p>
这是Ian Goodfellow,Aaron Courville和Youshua Bengio在2015出版的书。由于比较新，所以我准备利用这本书结合论文来学习。学习的同时做好笔记。
</p>
<div id="outline-container-sec-1" class="outline-2">
<h2 id="sec-1">应用数学与机器学习基础</h2>
<div class="outline-text-2" id="text-1">
</div><div id="outline-container-sec-1-1" class="outline-3">
<h3 id="sec-1-1">线性代数</h3>
<div class="outline-text-3" id="text-1-1">
<p>
前面的学过，但是感觉理解的不深入。比如特征向量、特征值的本质，这一部分还需要学习。主要计划是看网上<a href="http://v.163.com/special/opencourse/daishu.html">线性代数</a> 课程。同时也需要看对应的那本书，线性代数是很重要的。
</p>
</div>
<div id="outline-container-sec-1-1-1" class="outline-4">
<h4 id="sec-1-1-1">例子：主成份分析（PCA）</h4>
<div class="outline-text-4" id="text-1-1-1">
<p>
假设含有 \(m\) 个点的集合 \(\{x^{(1)},...,x^{(m)}\}\) 属于 \(R^n\) 。现在对这些点做有损压缩，即实现降维。这对输入为高维向量时非常有用，可以显著的降低运算。直观上来说，如果向量中有两个值的相关性很高，那么可以将它们中一个去掉，这可以作为一个解释。可以参考<a href="http://www.cnblogs.com/jerrylead/archive/2011/04/18/2020209.html">这里</a> 的解释。
</p>

<p>
一种方法：对每一个点 \(x^{(i)}\in R^n\) 找到一个对应的向量 \(c^{(i)}\in R^l\) 。其中 \(l\) 比 \(n\) 小。即找到两个函数 \(f(x)\) 和 \(g(c)\) 。
$$f(x)=c$$
$$x\approx g(f(x))$$
设 \(g(c)=Dc\) 。其中 \(D\) 是变换矩阵。PCA限制 \(D\) 的列向量是相互正交的,并假设每一向量是归一化的。
</p>

<p>
$D&isin; R<sup>n&times; l</sup>$，这表示 \(D\) 是一个将l维向量转化为n维向量的矩阵。这里，首先假设 \(D\) 矩阵已知，因此我们的目标是找到原向量 \(\vec{x}\) 对应的编码后的向量 $\vec{c}$，可以知道，这样的 \(\vec{c}\) 应该满足在所有的l维向量中，经过 \(D\) 矩阵的变换后与 $\vec{x} 距离最近：
$$c^*=min_c||\vec{x}-g(\vec{c})||)^2_2$$
最后解出:
$$\vec{c}=D^T\vec{x}$$
因此编码 \(\vec{x}\) 向量时：
$$f(\vec x)=D^T\vec x$$
复原操作，即从编码后的变换为编码前的向量时：
$$r(\vec x)=g(f(\vec x))=DD^T\vec x$$
通过上面的操作可以知道在变换矩阵已知时怎样编码原向量。
</p>

<p>
下一步就是选择编码矩阵 \(D\) 了。通过上一步我们知道每一个向量应该怎样变化使误差最小，而为了求编码矩阵，则需要总的误差最小：
$$D^*=argmin_D\sqrt{sum_{i,j}{(x_j^{(i)}-r({\vec x}^{(i)})_j)^2}}subject to D^TD=I_l$$
当假设映射到一维时，即l=1：
$$d^*=argmax_dTr({\vec d}^TX^TX\vec d)subject\space to\space {\vec d}^T{\vec d}=1$$
这个优化问题可以通过特征结构解决。
</p>

<p>
对于一般情况，当l&gt;1时，矩阵D是l个最大的特征值的对应的特征向量组成。
</p>

<p>
上面没有证明的中间过程，如果要实现推导则主要有以下几个：
</p>
<ul class="org-ul">
<li>已知重建矩阵，推导怎样编码一个输入向量使重建后的向量与原向量距离最近
</li>
<li>求重建矩阵，使重建后的向量距离之和最小
</li>
</ul>
</div>
</div>
</div>
</div>

<div id="outline-container-sec-2" class="outline-2">
<h2 id="sec-2">概率与信息论</h2>
<div class="outline-text-2" id="text-2">
<p>
在AI中，概率论主要应用于两种方式：
</p>
<ul class="org-ul">
<li>概率论准则告诉我们AI系统应该怎样推理，因此我们通过概率论来设计我们的算法来计算或最大化近似表达式。
</li>
<li>我们可以使用概率和统计来从理论上分析推出的AI系统的行为。
</li>
</ul>
</div>
<div id="outline-container-sec-2-1" class="outline-3">
<h3 id="sec-2-1">边缘概率</h3>
<div class="outline-text-3" id="text-2-1">
<p>
某些时候我们知道一个集合变量的概率分布，并且想知道某一个子集的概率分布。子集上的概率分布就是边缘概率分布。
例如，假设我们有离散随机变量X和Y，并且知道 \(P(x,y)\) 。我们可以通过 <i>sum rule</i> 来得到 \(P(x)\):
$$\forall x \in X,P(X=x)=\sum_yP(X=x,Y=y)$$
</p>
</div>
</div>
<div id="outline-container-sec-2-2" class="outline-3">
<h3 id="sec-2-2">条件概率</h3>
<div class="outline-text-3" id="text-2-2">
<p>
在许多情况下，我们对在某些事件已经发生的情况下另外某些事件发生的概率感兴趣。我们将在X=x的发生时Y=y的条件概率表示为 \(P(Y=y|X=x)\) 。
</p>

<p>
It is important not to confuse conditional probability with computing what would happen if some action were undertaken. The conditional probability thata person is from Germany given that they speak German is quite high, but if a randomly selected person is taught to speak German, their country of origin does not change. Computing the consequences of an action is called making an intervention query. Intervention queries are the domain of causal modeling, which we do not explore in this book.
</p>

<p>
上面这段话是说不要将条件概率和因果模型混淆，条件概率也是一种概率，是某种情况发生事另一种情况发生的概率，因果性指的是某种行为发生后一定引发的另一行为，即一个是描述的概率，不一定发生，一个描述因果性，是动作序列。
</p>
</div>
</div>

<div id="outline-container-sec-2-3" class="outline-3">
<h3 id="sec-2-3">独立与条件独立</h3>
<div class="outline-text-3" id="text-2-3">
<p>
两个变量X和Y独立的话：
$$\forall x \in X,y \in Y,p(X=x,Y=y)=p(X=x)p(Y=y)$$
当给一个随机变量Z，两个随机变量X和Y是条件独立时有如下分解：
$$\forall x \in X,y \in Y,z \in Z,p(X=x,Y=y|Z=z)=p(X=x|Z=z)p(Y=y|Z=z)$$
</p>
</div>
</div>

<div id="outline-container-sec-2-4" class="outline-3">
<h3 id="sec-2-4"><span class="todo TODO">TODO</span> 信息论(Infomation Theory)</h3>
<div class="outline-text-3" id="text-2-4">
<p>
信息论是应用数学的一个分支，In this context, information theory tells how to design optimal codes and calculate the expected length of messages sampled from specific probability distributions using various encoding schemes.
</p>

<p>
对于信息论的基本直觉是从不太可能发生而发生了的事件中学习到的知识比很可能发生而发生了的事件要多。信息用语言描述，那么常发生的用短的词，不常发生的词用长的词。对于离散信源而言，信源的概率空间的概率为 \(P(x_1),P(x_2),\dots,P(x_n)\) ，从信源输出一个消息提供的信息量就等于信源的不确定度。即从信源发出的各种消息的概率。当概率P越小，x消息出现的概率就越小，一旦出现所获得的信息量就越大。因此，定义：
$$I(x)=log(\frac 1 p(x))$$
称 \(I(x)\) 为消息x的自信息量，它具有随机变量的性质，但自信息量不能表示信源总体的不肯定度。自信息量表示一个消息出现后所带来的信息量，用其概率的负对数来表示，如上式。
</p>
</div>
</div>
<div id="outline-container-sec-2-5" class="outline-3">
<h3 id="sec-2-5">几个重要的密度函数</h3>
<div class="outline-text-3" id="text-2-5">
<p>
<i><b>logistic sigmoid</b></i> 
$$\sigma(x)=\frac 1 {1+exp(-x)}$$
<i><b>softplus function</b></i>
$$\zeta (x)=log(1+exp(x))$$
softplus函数可以用来产生高斯分布的 \(\beta\) 和 \(\sigma\) 参数，因为它的范围是 $R^+$。<br  />
以下是非常有用的性质：
$$\sigma (x)=\frac {exp(x)} {exp(x)+exp(0)}$$
$$\frac d {dx}\sigma(x)=\sigma(x)(1-\sigma(x))$$
$$1-\sigma(x)=\sigma(-x)$$
$$log \sigma(x)=-\zeta(-x)$$
$$\frac d {dx}\zeta (x)=\sigma(x)$$
$$\forall x \in (0,1),\sigma^{-1}(x)=log \Big( \frac x {1-x}\Big)$$
$$\forall x > 0,\zeta^{-1}(x)=log(exp(x)-1)$$
$$\zeta(x)=\int_{-\infty}^x\sigma(y)dy$$
$$\zeta(x)-\zeta(-x)=x$$
在统计学中 $&sigma;<sup>-1</sup>(x)称为 <i><b>logit</b></i> 。
</p>
</div>
</div>
<div id="outline-container-sec-2-6" class="outline-3">
<h3 id="sec-2-6"><span class="todo TODO">TODO</span> 测度论</h3>
<div class="outline-text-3" id="text-2-6">
<p>
在分形几何和概率论中都需要，应该尽快看一下。
</p>
</div>
</div>
<div id="outline-container-sec-2-7" class="outline-3">
<h3 id="sec-2-7"><span class="todo TODO">TODO</span> 概率图模型</h3>
<div class="outline-text-3" id="text-2-7">
<p>
机器学习算法包含了有大量随机变量的概率分布。但这些概率分布通常只被很少几个值直接影响。采用单个函数来描述整个概率分布会非常低效。我们可以将一个概率分布分解成许多因子，这样就可以降低难度。比如假设我们有三个随机变量a，b和c。假设a影响b，b影响c，在b给定时a和c是相互独立的。我们就可以将概率分布重新描述：
$$p(a,b,c)=p(a)p(b|a)p(c|b)$$
我们可以使用图来描述这些因子分解，这称为structured probabilistic model或图模型。
</p>
</div>
</div>
<div id="outline-container-sec-2-8" class="outline-3">
<h3 id="sec-2-8">朴素贝叶斯</h3>
<div class="outline-text-3" id="text-2-8">
<p>
朴素贝叶斯是一个简单的概率模型，通常使用在模式识别中。这个模型包含一个代表类型的随机变量c和一个代表每一类型对象特征的随机变量集合 \(F=\{f^{(1)},\dots ,f^{(n)}\}\) 。
</p>
</div>
</div>
</div>
<div id="outline-container-sec-3" class="outline-2">
<h2 id="sec-3">数值计算</h2>
<div class="outline-text-2" id="text-3">
</div><div id="outline-container-sec-3-1" class="outline-3">
<h3 id="sec-3-1">上溢和下溢</h3>
<div class="outline-text-3" id="text-3-1">
<p>
比如在机器学习中经常要将数加在一起，如果和太大就可能溢出。
</p>
</div>
</div>
<div id="outline-container-sec-3-2" class="outline-3">
<h3 id="sec-3-2">Poor Conditioning</h3>
<div class="outline-text-3" id="text-3-2">
<p>
科学计算中输入很小的变化会使函数变化很大会导致问题，因为输入的近似误差会使输出产生很大的变化。
</p>
</div>
</div>
<div id="outline-container-sec-3-3" class="outline-3">
<h3 id="sec-3-3"><span class="todo TODO">TODO</span> 基于梯度的最优化-实分析-包括梯度等的基本概念</h3>
<div class="outline-text-3" id="text-3-3">
<p>
大多数深度学习算法包括某种类型的最优化问题。最优化问题是通过改变 \(\vec x\) 来使函数 \(f(\vec x)\) 值最小或最大。我们想要最小化或最大化的函数被称为目标函数(objective function)或criterion。当我们最小化它时，我们也可以叫它花费函数(cost function)，损失函数(loss function)，或者错误函数(error function)。
</p>

<p>
在求函数最小值时，我们可以通过小步的移动x来使 \(f(x)\) 变小，这个x变化的方向是沿着梯度的反方向。这种技术被称为梯度下降(gradient descent)。
这会涉及全局最优和局部最优。在深度学习中，我们的优化函数可能有很多局部最优不是全局最优的。并且会有很多平的区域。因此我们通常寻找一个足够小但不必是最小的。
</p>
</div>
</div>
</div>
<div id="outline-container-sec-4" class="outline-2">
<h2 id="sec-4">机器学习基础</h2>
<div class="outline-text-2" id="text-4">
</div><div id="outline-container-sec-4-1" class="outline-3">
<h3 id="sec-4-1">学习算法</h3>
<div class="outline-text-3" id="text-4-1">
<p>
一个机器学习算法是一个可以从数据中学习的算法。
</p>
</div>
<div id="outline-container-sec-4-1-1" class="outline-4">
<h4 id="sec-4-1-1">任务，T</h4>
<div class="outline-text-4" id="text-4-1-1">
<p>
机器学习可以使我们解决很困难的问题，这些问题不能通过人设计实现的固定程序来解决。机器学习可以解决许多任务：
</p>
<ul class="org-ul">
<li><i><b>分类</b></i> : 比如对象识别，输入为一张图片（由代表亮度的像素集合描述），输入是一个指明物体类别的数字编码。
</li>
<li><i><b>聚类</b></i> 
</li>
<li><i><b>回归</b></i> : 和分类问题很像，但是输出的模式不一样。
</li>
<li><i><b>转换（Transcription）</b></i> ： 在这类任务中，机器学习系统被要求发现某些种类数据的相关的无结构的一种表述，并把它转化成离散的、文本的形式。例如在光学字符识别（OCR）中，输入给计算机程序一张有文本的图片，要求移字符序列的形式返回图片上的文本（例如以ASCII或Unicode形式）。另一个例子是语音识别，输入计算机的是声波，要求以字符序列的形式输出录音里的讲话。
</li>
<li><i><b>翻译</b></i>
</li>
<li><i><b>结构化输出（Structured output</b></i> : involve any task where the output is a vector containing important relationships between the different elements.这是很大一类，包含了上买能的转换和翻译，但还有其他的任务。一个例子是语法分析——将一个自然语言句子映射为一个语法分析树。 These tasks are called structured output tasks because the program must output several values that are all tightly inter-related. For example, the words produced by an image captioning program must form a valid sentence.
</li>
<li><i><b>异常检测</b></i> : 计算机程序过滤一个事件和对象的集合，标记其中不寻常或非典型的元素。
</li>
<li><i><b>综合与采样（Synthesis and sampling）</b></i> : 机器学习算法来生成和训练数据相识的新的例子。对于媒体应用非常有用，因为艺术家通过手工生成大量的内容是很昂贵和无趣的。
</li>
<li><i><b>丢失量归属</b></i> : 在这个任务中，机器学习的算法是给予一个新的例子 \(\vec x\in R^n\) 但是 \(\vec x\) 中的某些属性 \(x_i\) 丢失了，算法需要给丢失的属性提供一个预测值。
</li>
<li><i><b>去噪</b></i> : 这类任务中，输入为一个干净的例子被添加未知类型的噪音的有噪的例子，需要根据这个预测干净的例子，或给出条件概率。
</li>
<li><i><b>密度或概率函数估计</b></i> : 
</li>
</ul>

<p>
其他。
</p>
</div>
</div>
<div id="outline-container-sec-4-1-2" class="outline-4">
<h4 id="sec-4-1-2">性能度量，P</h4>
<div class="outline-text-4" id="text-4-1-2">
<p>
为了评估机器学习算法的能力，我们必须设计一个归于性能量化操作。
</p>
</div>
</div>
<div id="outline-container-sec-4-1-3" class="outline-4">
<h4 id="sec-4-1-3">经验，E</h4>
<div class="outline-text-4" id="text-4-1-3">
<p>
机器学习可以根据在学习期间得到经验的类型分为无监督和有监督的。
</p>

<p>
本书中的大多书的学习算法可以将经验理解为一个数据集。数据集是一个许多称为例子的对象的集合。
</p>

<p>
Unsupervised learning algorithms experience a dataset containing many features,then learn useful properties of the structure of this dataset. In the context of deep learning, we usually want to learn the entire probability distribution that generated a dataset, whether explicitly as in density estimation or implicitly for tasks like synthesis or denoising. Some other unsupervised learning algorithms perform other roles, like dividing the dataset into clusters of similar examples.
</p>

<p>
Supervised learning algorithms experience a dataset containing features, but each example is also associated with a label or target. For example, the Iris dataset is annotated with the species of each iris plant. A supervised learning algorithm can study the Iris dataset and learn to classify iris plants into three different species based on their measurements.
</p>

<p>
Roughly speaking, unsupervised learning involves observing several examples of a random vector x, and attempting to implicitly or explicitly learn the probability distribution p(x), or some interesting properties of that distribution, while supervised learning involves observing several examples of a random vector x and an associated value or vector y , and learning to predicty from x, e.g. estimating p(y | x). The term supervised learning originates from the view of the target y being provided by an instructor or teacher that shows the machine learning system what to do. In unsupervised learning, there is no instructor or teacher, and the algorithm must learn to make sense of the data without this guide.
</p>
</div>
</div>
</div>

<div id="outline-container-sec-4-2" class="outline-3">
<h3 id="sec-4-2">容量、过拟合、欠拟合</h3>
<div class="outline-text-3" id="text-4-2">
<p>
将机器学习与最优化i问题区分开来的是机器学习需要generalization error，或者被叫做测试错误来最小化。怎样仅仅通过训练集和来影响对测试集合的性能？统计学习理论提供了某些答案。
</p>

<p>
We can control whether a model is more likely to overfit or underfit by altering its capacity. Informally, a model’s capacity is its ability to fit a wide variety of functions. Models with low capacity may struggle to fit the training set. Models with high capacity can overfit, i.e., memorize properties of the training set that do not serve them well on the test set.
</p>

<p>
参数化的模型如线性回归，非参数化的模型如最临近算法。
</p>
</div>

<div id="outline-container-sec-4-2-1" class="outline-4">
<h4 id="sec-4-2-1">没有免费的午餐理论（The No Free Lunch Theorem）</h4>
<div class="outline-text-4" id="text-4-2-1">
<p>
The no free lunch theorem for machine learning (Wolpert, 1996) states that, averaged over all possible data generating distributions, every classification algorithm has the same errorrate when classifying previously unobserved points. In other words, in some sense,no machine learning algorithm is universally any better than any other. The most sophisticated algorithm we can conceive of has the same average performance (over all possible tasks) as merely predicting that every point belongs to the same class.
这表明没有一个通用的机器学习算法可以对所有任务有好的效果。因此我们要做的是对某一具体的任务设计好的算法，如果我们可以假设真是世界中的应用的概率分布，我们就可以有针对的设计算法。因此机器学习研究的目的不是找到一个针对所有问题的最好的算法，相反，我们的目标是选择那种机器学习算法对我们关心的数据有好的效果。
</p>
</div>
</div>
</div>
<div id="outline-container-sec-4-3" class="outline-3">
<h3 id="sec-4-3">类型参数与验证集</h3>
<div class="outline-text-3" id="text-4-3">
<p>
大多数机器学习算法都有几个设置，使用这几个设置可以来控制学习算法的行为。这些设置就是类型参数。类型参数的值不是通过学习算法自己来调整的（因此要调参）。比如调节过拟合的 \(\lambda\)
</p>

<p>
我们总是从训练集里构造验证集。我们将训练数据分成不相关的两部分。其中一个子集用来学习参数。另一个自己就是验证集，用来估计测试时的测试误差，使得类型参数能够相应得到更新。用来指导选择类型参数的子集被称为验证集。
</p>
</div>
<div id="outline-container-sec-4-3-1" class="outline-4">
<h4 id="sec-4-3-1">交叉验证</h4>
<div class="outline-text-4" id="text-4-3-1">
<p>
当数据很少时，测试数据会很少，k-fold cross-validation。
</p>
</div>
</div>
</div>
<div id="outline-container-sec-4-4" class="outline-3">
<h3 id="sec-4-4">估计，偏差和方差</h3>
<div class="outline-text-3" id="text-4-4">
<p>
在机器学习中应用统计学工具。
</p>
</div>
<div id="outline-container-sec-4-4-1" class="outline-4">
<h4 id="sec-4-4-1">点估计</h4>
<div class="outline-text-4" id="text-4-4-1">
<p>
注意，估计量是随机变量的一种组合，因此也是随机变量。
</p>
</div>
</div>
<div id="outline-container-sec-4-4-2" class="outline-4">
<h4 id="sec-4-4-2">偏差</h4>
<div class="outline-text-4" id="text-4-4-2">
<p>
因为估计量是随机变量，我们可以判断这个随机变量的期望，如果估计量的期望等于真是值，则是无偏的。
</p>

<p>
对于方差，我们有两种估计量，一个是有偏，一个无偏，但是它们都不是最好的估计量。我们通常使用有偏的估计量，因为它有一些比较重要的性质。
</p>
</div>
</div>
<div id="outline-container-sec-4-4-3" class="outline-4">
<h4 id="sec-4-4-3">方差和标准差</h4>
<div class="outline-text-4" id="text-4-4-3">
<p>
除了可以计算估计量的期望，还可以计算它的方差。对于方差的估计量，虽然第一个是有偏的，但是它的方差比第二个无偏的小。
</p>
</div>
</div>
<div id="outline-container-sec-4-4-4" class="outline-4">
<h4 id="sec-4-4-4">平衡（trade off）：偏差与方差与均方误差</h4>
<div class="outline-text-4" id="text-4-4-4">
<p>
偏差和方差是衡量两个不同估计量的的两种方式。那么对于两个估计量，一个偏差大，一个方差大，应该选择哪一个呢？
</p>

<p>
在机器学习中，最通用的也从经验上来说成功的方式是平衡方式，一般是交叉验证。另一种是我们可以比较估计量的均方误差（mean squared error）：
</p>
\begin{eqnarray*}
MSE &=&E[({\hat \theta}_n-\theta)^2]\\
&=&Bias({\hat \theta}_n)^2+Var({\hat \theta}_n)
\end{eqnarray*}
</div>
</div>
<div id="outline-container-sec-4-4-5" class="outline-4">
<h4 id="sec-4-4-5">一致性（Consistency）</h4>
<div class="outline-text-4" id="text-4-4-5">
<p>
我们希望当我们的测试数据增加时，我们的预测值可以集中到真实值上，即：
$$lim_{n \to \infty}{\hat \theta}_n \stackrel{p}{\longrightarrow} \theta^2$$
不偏不等于一致。比如假设预估计量就是第一个随机变量，则这个估计量一定是无偏的，但是这却不满足一致性。
</p>
</div>
</div>
</div>
<div id="outline-container-sec-4-5" class="outline-3">
<h3 id="sec-4-5">最大似然估计</h3>
<div class="outline-text-3" id="text-4-5">
<p>
前面我们定义了一些估计量和分析了它们的属性。但是这些估计量是哪里来的？不是通过猜测某些函数可能是一个好的估计量然后分析它的偏差和方差，我们想要某些方法可以是我们对于不同的模型都可以得到好的估计量的函数。最通用的方法是最大似然法则。这里是指已知模型而对模型的参数进行估计。
</p>


<hr  />
<a href="http://oyzh.github.io">Back to Homepage</a>
<br>
<a href="http://github.com/oyzh">Github</a>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="validation"></p>
</div>
</body>
</html>
