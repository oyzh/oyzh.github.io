#+HTML_MATHJAX: align:"center" mathml:t path:"http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"
#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="/css/style.css">
#+OPTIONS: author:nil
#+OPTIONS: creator:nil
#+OPTIONS: timestamp:nil
-----
#+TITLE:Deep Learning
#+OPTIONS: toc:nil
#+OPTIONS: num:nil
这是Ian Goodfellow,Aaron Courville和Youshua Bengio在2015出版的书。由于比较新，所以我准备利用这本书结合论文来学习。学习的同时做好笔记。
* 应用数学与机器学习基础
** 线性代数
前面的学过，但是感觉理解的不深入。比如特征向量、特征值的本质，这一部分还需要学习。主要计划是看网上[[http://v.163.com/special/opencourse/daishu.html][线性代数]] 课程。同时也需要看对应的那本书，线性代数是很重要的。
*** 例子：主成份分析（PCA）
假设含有 \(m\) 个点的集合 \(\{x^{(1)},...,x^{(m)}\}\) 属于 \(R^n\) 。现在对这些点做有损压缩，即实现降维。这对输入为高维向量时非常有用，可以显著的降低运算。直观上来说，如果向量中有两个值的相关性很高，那么可以将它们中一个去掉，这可以作为一个解释。可以参考[[http://www.cnblogs.com/jerrylead/archive/2011/04/18/2020209.html][这里]] 的解释。

一种方法：对每一个点 $x^{(i)}\in R^n$ 找到一个对应的向量 $c^{(i)}\in R^l$ 。其中 $l$ 比 $n$ 小。即找到两个函数 $f(x)$ 和 $g(c)$ 。
$$f(x)=c$$
$$x\approx g(f(x))$$
设 $g(c)=Dc$ 。其中 $D$ 是变换矩阵。PCA限制 $D$ 的列向量是相互正交的,并假设每一向量是归一化的。

$D\in R^{n\times l}$，这表示 $D$ 是一个将l维向量转化为n维向量的矩阵。这里，首先假设 $D$ 矩阵已知，因此我们的目标是找到原向量 $\vec{x}$ 对应的编码后的向量 $\vec{c}$，可以知道，这样的 $\vec{c}$ 应该满足在所有的l维向量中，经过 $D$ 矩阵的变换后与 $\vec{x} 距离最近：
$$c^*=min_c||\vec{x}-g(\vec{c})||)^2_2$$
最后解出:
$$\vec{c}=D^T\vec{x}$$
因此编码 $\vec{x}$ 向量时：
$$f(\vec x)=D^T\vec x$$
复原操作，即从编码后的变换为编码前的向量时：
$$r(\vec x)=g(f(\vec x))=DD^T\vec x$$
通过上面的操作可以知道在变换矩阵已知时怎样编码原向量。

下一步就是选择编码矩阵 $D$ 了。通过上一步我们知道每一个向量应该怎样变化使误差最小，而为了求编码矩阵，则需要总的误差最小：
$$D^*=argmin_D\sqrt{sum_{i,j}{(x_j^{(i)}-r({\vec x}^{(i)})_j)^2}}subject to D^TD=I_l$$
当假设映射到一维时，即l=1：
$$d^*=argmax_dTr({\vec d}^TX^TX\vec d)subject\space to\space {\vec d}^T{\vec d}=1$$
这个优化问题可以通过特征结构解决。

对于一般情况，当l>1时，矩阵D是l个最大的特征值的对应的特征向量组成。

上面没有证明的中间过程，如果要实现推导则主要有以下几个：
+ 已知重建矩阵，推导怎样编码一个输入向量使重建后的向量与原向量距离最近
+ 求重建矩阵，使重建后的向量距离之和最小
  
* 概率与信息论
在AI中，概率论主要应用于两种方式：
+ 概率论准则告诉我们AI系统应该怎样推理，因此我们通过概率论来设计我们的算法来计算或最大化近似表达式。
+ 我们可以使用概率和统计来从理论上分析推出的AI系统的行为。
** 边缘概率
某些时候我们知道一个集合变量的概率分布，并且想知道某一个子集的概率分布。子集上的概率分布就是边缘概率分布。
例如，假设我们有离散随机变量X和Y，并且知道 $P(x,y)$ 。我们可以通过 /sum rule/ 来得到 $P(x)$:
$$\forall x \in X,P(X=x)=\sum_yP(X=x,Y=y)$$
** 条件概率
在许多情况下，我们对在某些事件已经发生的情况下另外某些事件发生的概率感兴趣。我们将在X=x的发生时Y=y的条件概率表示为 $P(Y=y|X=x)$ 。

It is important not to confuse conditional probability with computing what would happen if some action were undertaken. The conditional probability thata person is from Germany given that they speak German is quite high, but if a randomly selected person is taught to speak German, their country of origin does not change. Computing the consequences of an action is called making an intervention query. Intervention queries are the domain of causal modeling, which we do not explore in this book.

上面这段话是说不要将条件概率和因果模型混淆，条件概率也是一种概率，是某种情况发生事另一种情况发生的概率，因果性指的是某种行为发生后一定引发的另一行为，即一个是描述的概率，不一定发生，一个描述因果性，是动作序列。

** 独立与条件独立
两个变量X和Y独立的话：
$$\forall x \in X,y \in Y,p(X=x,Y=y)=p(X=x)p(Y=y)$$
当给一个随机变量Z，两个随机变量X和Y是条件独立时有如下分解：
$$\forall x \in X,y \in Y,z \in Z,p(X=x,Y=y|Z=z)=p(X=x|Z=z)p(Y=y|Z=z)$$

** TODO 信息论(Infomation Theory)
信息论是应用数学的一个分支，In this context, information theory tells how to design optimal codes and calculate the expected length of messages sampled from specific probability distributions using various encoding schemes.

对于信息论的基本直觉是从不太可能发生而发生了的事件中学习到的知识比很可能发生而发生了的事件要多。信息用语言描述，那么常发生的用短的词，不常发生的词用长的词。对于离散信源而言，信源的概率空间的概率为 $P(x_1),P(x_2),\dots,P(x_n)$ ，从信源输出一个消息提供的信息量就等于信源的不确定度。即从信源发出的各种消息的概率。当概率P越小，x消息出现的概率就越小，一旦出现所获得的信息量就越大。因此，定义：
$$I(x)=log(\frac 1 p(x))$$
称 $I(x)$ 为消息x的自信息量，它具有随机变量的性质，但自信息量不能表示信源总体的不肯定度。自信息量表示一个消息出现后所带来的信息量，用其概率的负对数来表示，如上式。
** 几个重要的密度函数
/*logistic sigmoid*/ 
$$\sigma(x)=\frac 1 {1+exp(-x)}$$
/*softplus function*/
$$\zeta (x)=log(1+exp(x))$$
softplus函数可以用来产生高斯分布的 $\beta$ 和 $\sigma$ 参数，因为它的范围是 $R^+$。\\
以下是非常有用的性质：
$$\sigma (x)=\frac {exp(x)} {exp(x)+exp(0)}$$
$$\frac d {dx}\sigma(x)=\sigma(x)(1-\sigma(x))$$
$$1-\sigma(x)=\sigma(-x)$$
$$log \sigma(x)=-\zeta(-x)$$
$$\frac d {dx}\zeta (x)=\sigma(x)$$
$$\forall x \in (0,1),\sigma^{-1}(x)=log \Big( \frac x {1-x}\Big)$$
$$\forall x > 0,\zeta^{-1}(x)=log(exp(x)-1)$$
$$\zeta(x)=\int_{-\infty}^x\sigma(y)dy$$
$$\zeta(x)-\zeta(-x)=x$$
在统计学中 $\sigma^{-1}(x)称为 /*logit*/ 。
** TODO 测度论
在分形几何和概率论中都需要，应该尽快看一下。
** TODO 概率图模型
机器学习算法包含了有大量随机变量的概率分布。但这些概率分布通常只被很少几个值直接影响。采用单个函数来描述整个概率分布会非常低效。我们可以将一个概率分布分解成许多因子，这样就可以降低难度。比如假设我们有三个随机变量a，b和c。假设a影响b，b影响c，在b给定时a和c是相互独立的。我们就可以将概率分布重新描述：
$$p(a,b,c)=p(a)p(b|a)p(c|b)$$
我们可以使用图来描述这些因子分解，这称为structured probabilistic model或图模型。
** 朴素贝叶斯
朴素贝叶斯是一个简单的概率模型，通常使用在模式识别中。这个模型包含一个代表类型的随机变量c和一个代表每一类型对象特征的随机变量集合 $F=\{f^{(1)},\dots ,f^{(n)}\}$ 。
* 数值计算
** 上溢和下溢
比如在机器学习中经常要将数加在一起，如果和太大就可能溢出。
** Poor Conditioning
科学计算中输入很小的变化会使函数变化很大会导致问题，因为输入的近似误差会使输出产生很大的变化。
** TODO 基于梯度的最优化-实分析-包括梯度等的基本概念
大多数深度学习算法包括某种类型的最优化问题。最优化问题是通过改变 $\vec x$ 来使函数 $f(\vec x)$ 值最小或最大。我们想要最小化或最大化的函数被称为目标函数(objective function)或criterion。当我们最小化它时，我们也可以叫它花费函数(cost function)，损失函数(loss function)，或者错误函数(error function)。

在求函数最小值时，我们可以通过小步的移动x来使 $f(x)$ 变小，这个x变化的方向是沿着梯度的反方向。这种技术被称为梯度下降(gradient descent)。
这会涉及全局最优和局部最优。在深度学习中，我们的优化函数可能有很多局部最优不是全局最优的。并且会有很多平的区域。因此我们通常寻找一个足够小但不必是最小的。
-----
#+BEGIN_HTML
<a href="http://oyzh.github.io">Back to Homepage</a>
<br>
<a href="http://github.com/oyzh">Github</a>
#+END_HTML
