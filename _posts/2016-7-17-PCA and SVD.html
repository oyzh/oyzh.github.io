---
layout: post
title: 主成分分析(PCA)与奇异值分解(SVD)
excerpt: 介绍PCA和SVD
---
<div id="outline-container-sec-1" class="outline-2">
<h2 id="sec-1">PCA</h2>
<div class="outline-text-2" id="text-1">
<p>
主成分分析是从统计数据(或训练数据)找到降低数据维度的方法。PCA的根本其实只是一个简单的不同基下的坐标变换。它也不是直接就将维度降低，而是在其他基下数据在某些维度上的数据的方差很小，因此可以忽略那些维度上的数据，从而实现降维。
</p>

<p>
方差体现的是数据离均值的分散程度。变量之间的协方差体现的是两变量的线性相关性。我们想要在新的基下数据协方差为0,最后为了实现降维而只使用方差递减的前k项，因为方差小说明在这一个维度上数据差异不明显。
</p>

<p>
设训练数据为X，它是n×d的矩阵，n是数据个数，d是数据维度。要得到一个变换矩阵P将X转化为同样d维的Y(Y也是n×d矩阵)。
$$Y^T=PX^T$$
也就是将X中每一个向量转化成Y中对应的向量，因此P是d×d矩阵。
</p>

<p>
PCA的主要目的是通过这样的P得到的Y所有协方差都为0，Y的协方差(假设已经使每一维度均值为0):
$$C_Y=Y^T*Y$$
同理X的协方差为：
$$C_X=X^T*X$$
约束条件是 \(C_Y\) 是对角矩阵。目标是求得P。通过计算可以知道P是X的协方差矩阵的特征向量，因此问题变为求训练数据的协方差矩阵并求这个矩阵的特征向量的问题，同时为了实现降维可以选取特征值最大的几项对应的特征向量最为最后的变换矩阵P'，直接用P'乘以一个测试向量就可以得到一个低维的向量。
</p>
</div>
</div>

<div id="outline-container-sec-2" class="outline-2">
<h2 id="sec-2">SVD</h2>
<div class="outline-text-2" id="text-2">
<p>
在PCA中需要求协方差矩阵然后求协方差矩阵的特征值和特征向量，而利用SVD可以通过分解 \(X^T\) 来计算，不用直接求协方差矩阵。
</p>

<p>
SVD是对于任意的矩阵A(m×n)有：
$$A=SVD^T$$
其中有 \(S^T*S=E\) 和 \(D^T*D=E\) 。具体计算可以参考书本。
</p>
</div>
</div>
